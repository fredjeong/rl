# Multi-armed Bandits

1. [Introduction](./introduction.md)
2. [A k-armed Bandit Problem](./k-armed-bandit-problem.md)
3. [Action-value Methods](./action-value-methods.md)
4. [The 10-armed Testbed](./the-10-armed-testbed.ipynb)
5. [Incremental Implementation](./incremental-implementation.md)
6. [Tracking a Nonstationary Problem](./tracking-a-nonstationary-problem.md)
7. [Optimistic Initial Values](./optimistic-initial-values.md)
8. [Upper-Confidence-Bound Action Selection](./upper-confidence-bound-action-selection.md)
9. [Gradient Bandit Algorithms](./gradient-bandit-algorithms.md)
10. [Associative Search (Contextual Bandits)](./associative-search.md)
