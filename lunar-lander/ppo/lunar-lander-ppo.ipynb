{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimisation on LunarLanderContinuous-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theoretical Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common estimator for policy gradient at time step $t$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\nabla_\\theta \\hat{J(\\theta)} = \\hat {\\mathbb E}_t\\left[ \\nabla_\\theta\\log \\pi_\\theta(a_t\\mid s_t) \\hat A_t \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\hat A_t$ is an estimator of the advantage function at time step $t$. (Please refer to the [A2C Notebook](../a2c/lunar-lander-a2c.ipynb) for deriving the above estimator).\n",
    "\n",
    "In this case, the estimator is obtained by differentiating the objective function\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\hat {\\mathbb E}_t\\left[ \\log \\pi_\\theta(a_t\\mid s_t)\\hat A_t \\right].\n",
    "$$\n",
    "\n",
    "One of the drawbacks of the above method, it leads to \"destructively large\" policy updates, making our gradient estimates have high variance, meaning that the gradient estimates can fluctuate significantly from one batch of samples to another. This leads to the update of the parameters $\\theta$ erratic and unstable.\n",
    "\n",
    "Hence TRPO is suggested ([Schulman, 2015]()). The objective is maximised subject to a constraint on the size of the policy update:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\text{maximise}_\\theta& \\hat {\\mathbb E}_t\\left[ \\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)} \\hat A_t \\right]\n",
    "\\\\\n",
    "\\text{subject to} & \\hat {\\mathbb E}_t\\left[ KL\\left[ \\pi_{\\theta_{old}}(\\cdot\\mid s_t), \\pi_\\theta(\\cdot\\mid s_t) \\right] \\right] \\leq \\delta\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "for some hyperparameter $\\delta$. It used probability ratio, which compares the probability of taking an action under the new and the old policy. Now it becomes an optimisation problem under under a constraint, which prevents the new policy to deviate too much from an old policy, contributing to more stable learning. If $\\hat A_t$ is positive, we want to increase the probability of taking $a_t$ in state $s_t$. The objective function $\\hat{\\mathbb E}_t\\left[ \\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)} \\hat A_t \\right]$ achieves this by making the probability ratio greater than 1, which increases the overall value of the expectation. On the other hand, if $\\hat A_t$ is negative, we want to minimise the absolute value of our estimate of expectation, which is done by making the probability ratio less than 1.\n",
    "\n",
    "Although this setting makes the overall learning process quite robust, it leaves a room for improvement. The biggest problem of TRPO is that it is inefficient. Calculating the KL divergence between the new and the old policies requires second-order optimisation techniques such as conjugate gradient and Hessian-vector products, which can be computationally expensive. PPO ([Schulman, 2017]()) overcomes this issue by introducing a clipped surrogate objective:\n",
    "\n",
    "$$\n",
    "L^{CLIP}(\\theta) = \\min\\left( r_t(\\theta) \\hat A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat A_t \\right)\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a hyperparameter and $r_t(\\theta)=\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)}$ is the probabiltiy ratio. Trivially, we have $r_t(\\theta_{old})=1$. Note that the constraint is directly integrated in the form of \"clip\" function. That is, whenever the probability ratio is greater or less than our pre-specified threshold, we \"clip\" and force the value to be that threshold. It is much simpler to implement, as it only requires first-order optimisation methods.\n",
    "\n",
    "The objective function in PPO is:\n",
    "\n",
    "$$\n",
    "L_t^{CLIP+VF+S}(\\theta) = \\hat{\\mathbb E}_t \\left[ L_t^{CLIP}(\\theta) - c_1 L_t^{VF} + c_2 S[\\pi_\\theta](s_t) \\right]\n",
    "$$\n",
    "\n",
    "where $c_1$, $c_2$ are coefficients, $S$  is the entropy bonus, and $L_t^{VF}$ is a MSE loss of the value function. We already covered what $L_t^{CLIP}$, so now we figure out what the other two terms are.\n",
    "\n",
    "Note that we have an advantage estimate $\\hat A_t$ in $L_t^{CLIP}$. This advantage estimate ([Minh et al., 2016]()) is defined as\n",
    "\n",
    "$$\n",
    "\\hat A_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma \\lambda)^{T-t+1}\\delta_{T-1}\n",
    "$$\n",
    "\n",
    "where $\\delta_t = r_t+\\gamma V(s_{t+1}) - V(s_t)$. To estimate the advantage function, the value functon is crucial. A more accurate value function leads to better advantage estimates and, consequently, better policy updates. Therefore, we train the value function alongside the policy. \n",
    "\n",
    "The last term, $S[\\pi_\\theta](s_t)$ represents the entropy ([Williams, 1992]()) of the policy $\\pi_\\theta$ at state $s_t$. Defined as\n",
    "\n",
    "$$\n",
    "S[\\pi_\\theta](s_t) \n",
    "=\n",
    "\\begin{cases}\n",
    "-\\int_{a\\in\\mathcal A}\\pi_\\theta(a\\mid s_t) \\log_2 \\pi_\\theta(a\\mid s_t) da & \\mathcal A \\text{ is continuous}\n",
    "\\\\\n",
    "-\\sum_{a\\in\\mathcal A}\\pi_\\theta(a\\mid s_t) \\log_2 \\pi_\\theta(a\\mid s_t) & \\mathcal A \\text{ is discrete}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "entropy quantifies how unpredictable a policy's actions are. By adding an entropy bonus to the loss function, we incentivise the agent to explore a wide range of actions, helping prevent the policy from converging too quickly to a suboptimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ppo-algorithm](../../figures/ppo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define actor and critic networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "\t\tself.log_stds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "\t\tself.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\tself.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tx = F.relu(self.fc1(state))\n",
    "\t\tx = F.relu(self.fc2(torch.cat([x, action], dim=1)))\n",
    "\t\tx = self.fc3(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "\tdef __init__(self, action_dim, memory_size, batch_size):\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.memory_size = memory_size\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.memory = deque(maxlen=memory_size)\n",
    "\t\tself.experience = namedtuple(\n",
    "\t\t\t\t\t\t\t\"Experience\", \n",
    "\t\t\t\t   \t\t\tfield_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"log_prob\", \"val\"])\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.memory)\n",
    "\t\n",
    "\tdef push(self, state, action, reward, next_state, done, log_prob, val):\n",
    "\t\te = self.experience(state, action, reward, next_state, done, log_prob, val)\n",
    "\t\tself.memory.append(e)\n",
    "\t\n",
    "\tdef sample(self):\n",
    "\t\texperiences = random.sample(self.memory, k=self.batch_size)\n",
    "\t\tstates = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "\t\tactions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "\t\trewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "\t\tnext_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "\t\tdones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\t\tlog_probs = torch.from_numpy(np.vstack([e.log_prob for e in experiences if e is not None])).float().to(device)\n",
    "\t\tvals = torch.from_numpy(np.vstack([e.val for e in experiences if e is not None])).float().to(device)\n",
    "\t\t\n",
    "\t\treturn (states, actions, rewards, next_states, dones, log_probs, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "\tdef __init__(self,):\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef act(self, state):\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef save_to_memory(self,):\n",
    "\t\tpass\n",
    "\n",
    "\tdef optimize(self,):\n",
    "\t\tpass\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "train_env = gym.make(\"LunarLanderContinuous-v3\")\n",
    "state_dim = train_env.observation_space.shape[0]\n",
    "action_dim = train_env.action_space.shape[0]\n",
    "target_score = 250\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "n_episodes = 5000\n",
    "batch_size = 64\n",
    "memory_size = T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes, max_steps, gamma, target):\n",
    "\tscore_history = []\n",
    "\t\n",
    "\tbar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
    "\tprogress_bar = tqdm.trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
    "\n",
    "\tfor epsode in progress_bar:\n",
    "\t\tstate, _ = env.reset()\n",
    "\n",
    "\t\tscore = 0\n",
    "\t\t\n",
    "\t\tfor t in range(max_steps):\n",
    "\t\t\t# Run policy for T timesteps\n",
    "\t\t\taction = agent.act(state)\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\t\t\tlog_prob =\n",
    "\t\t\tif done:\n",
    "\t\t\t\tval = 0\n",
    "\t\t\t\tbreak\n",
    "\t\t\tval = \n",
    "\t\t\tagent.save_to_memory(state, action, reward, next_state, done, log_prob, val)\n",
    "\t\t\t\n",
    "\t\t\t# Compute advantage estimates (A_1, ..., A_t)\n",
    "\t\t\t\n",
    "\n",
    "\t\t# Opitmise surrogate L w.r.t. theta, with K epochs and minibatch size M <= T\n",
    "\t\tagent.optimize()\n",
    "\t\t\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise actor, critic, and replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here actor network will output action probabilities and critc network will output Q value for a given state(We don't need to pass action and state to critc network since we found above that calculating advanatge depends up on states).\n",
    "\n",
    "Data store is the memory where we store our state,action,action probability and rewards. It is also able to generate batches of data for traning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each time step in the episode we store current_state, action, action probability, Q value(output from critic network for that state), reward and done status in the Datastore(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now when we reach required number of steps in the episode, we will train our model.With already generated batches we will start our traning for n_epoch. For each epoch we will do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the advantage array using the reward array and Q value array (we are saying array because we are having a batch of data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
