{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimisation on LunarLanderContinuous-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theoretical Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common estimator for policy gradient at time step $t$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\nabla_\\theta \\hat{J(\\theta)} = \\hat {\\mathbb E}_t\\left[ \\nabla_\\theta\\log \\pi_\\theta(a_t\\mid s_t) \\hat A_t \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\hat A_t$ is an estimator of the advantage function at time step $t$. (Please refer to the [A2C Notebook](../a2c/lunar-lander-a2c.ipynb) for deriving the above estimator).\n",
    "\n",
    "In this case, the estimator is obtained by differentiating the objective function\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\hat {\\mathbb E}_t\\left[ \\log \\pi_\\theta(a_t\\mid s_t)\\hat A_t \\right].\n",
    "$$\n",
    "\n",
    "One of the drawbacks of the above method, it leads to \"destructively large\" policy updates, making our gradient estimates have high variance, meaning that the gradient estimates can fluctuate significantly from one batch of samples to another. This leads to the update of the parameters $\\theta$ erratic and unstable.\n",
    "\n",
    "Hence TRPO is suggested ([Schulman, 2015]()). The objective is maximised subject to a constraint on the size of the policy update:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\text{maximise}_\\theta& \\hat {\\mathbb E}_t\\left[ \\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)} \\hat A_t \\right]\n",
    "\\\\\n",
    "\\text{subject to} & \\hat {\\mathbb E}_t\\left[ KL\\left[ \\pi_{\\theta_{old}}(\\cdot\\mid s_t), \\pi_\\theta(\\cdot\\mid s_t) \\right] \\right] \\leq \\delta\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "for some hyperparameter $\\delta$. It used probability ratio, which compares the probability of taking an action under the new and the old policy. Now it becomes an optimisation problem under under a constraint, which prevents the new policy to deviate too much from an old policy, contributing to more stable learning. If $\\hat A_t$ is positive, we want to increase the probability of taking $a_t$ in state $s_t$. The objective function $\\hat{\\mathbb E}_t\\left[ \\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)} \\hat A_t \\right]$ achieves this by making the probability ratio greater than 1, which increases the overall value of the expectation. On the other hand, if $\\hat A_t$ is negative, we want to minimise the absolute value of our estimate of expectation, which is done by making the probability ratio less than 1.\n",
    "\n",
    "Although this setting makes the overall learning process quite robust, it leaves a room for improvement. The biggest problem of TRPO is that it is inefficient. Calculating the KL divergence between the new and the old policies requires second-order optimisation techniques such as conjugate gradient and Hessian-vector products, which can be computationally expensive. PPO ([Schulman, 2017]()) overcomes this issue by introducing a clipped surrogate objective:\n",
    "\n",
    "$$\n",
    "L^{CLIP}(\\theta) = \\min\\left( r_t(\\theta) \\hat A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat A_t \\right)\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a hyperparameter and $r_t(\\theta)=\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)}$ is the probabiltiy ratio. Trivially, we have $r_t(\\theta_{old})=1$. Note that the constraint is directly integrated in the form of \"clip\" function. That is, whenever the probability ratio is greater or less than our pre-specified threshold, we \"clip\" and force the value to be that threshold. It is much simpler to implement, as it only requires first-order optimisation methods.\n",
    "\n",
    "The objective function in PPO is:\n",
    "\n",
    "$$\n",
    "L_t^{CLIP+VF+S}(\\theta) = \\hat{\\mathbb E}_t \\left[ L_t^{CLIP}(\\theta) - c_1 L_t^{VF} + c_2 S[\\pi_\\theta](s_t) \\right]\n",
    "$$\n",
    "\n",
    "where $c_1$, $c_2$ are coefficients, $S$  is the entropy bonus, and $L_t^{VF}$ is a MSE loss of the value function. We already covered what $L_t^{CLIP}$, so now we figure out what the other two terms are.\n",
    "\n",
    "Note that we have an advantage estimate $\\hat A_t$ in $L_t^{CLIP}$. This advantage estimate ([Minh et al., 2016]()) is defined as\n",
    "\n",
    "$$\n",
    "\\hat A_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma \\lambda)^{T-t+1}\\delta_{T-1}\n",
    "$$\n",
    "\n",
    "where $\\delta_t = r_t+\\gamma V(s_{t+1}) - V(s_t)$. To estimate the advantage function, the value functon is crucial. A more accurate value function leads to better advantage estimates and, consequently, better policy updates. Therefore, we train the value function alongside the policy. \n",
    "\n",
    "The last term, $S[\\pi_\\theta](s_t)$ represents the entropy ([Williams, 1992]()) of the policy $\\pi_\\theta$ at state $s_t$. Defined as\n",
    "\n",
    "$$\n",
    "S[\\pi_\\theta](s_t) \n",
    "=\n",
    "\\begin{cases}\n",
    "-\\int_{a\\in\\mathcal A}\\pi_\\theta(a\\mid s_t) \\log_2 \\pi_\\theta(a\\mid s_t) da & \\mathcal A \\text{ is continuous}\n",
    "\\\\\n",
    "-\\sum_{a\\in\\mathcal A}\\pi_\\theta(a\\mid s_t) \\log_2 \\pi_\\theta(a\\mid s_t) & \\mathcal A \\text{ is discrete}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "entropy quantifies how unpredictable a policy's actions are. By adding an entropy bonus to the loss function, we incentivise the agent to explore a wide range of actions, helping prevent the policy from converging too quickly to a suboptimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ppo-algorithm](../../figures/ppo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
