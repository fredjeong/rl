{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network on LunarLander-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project we introduce theoretical concepts behind Deep Q-Network, a powerful reinforcement learning technique, and detail the implementation and performance of our agent in the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theoretical Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "Value-based methods focus on estimating and using the state-value function $V(s)$ or the action-value function $Q(s,a)$ to make decision and learn optimal policies. A representative method is Q-learning introduced by [Watkins (1989)]() and further developed by [Watkins and Dayan (1992)](). Instead of trying to estimate the transition function $P_t(s'\\mid s,a)$ we focus on learning the action-value function directly.\n",
    "\n",
    "Suppose we take an action $a$ given an initial state $s$. We receive a reward $r(s,a)$ and observe a new state $s'$. Under the optimal policy, we have\n",
    "\n",
    "$$\n",
    "Q(s,a) = r(s,a) + V^*(s')\n",
    "$$\n",
    "\n",
    "where $V^*$ is the maximum state-value under the optimal policy. We can also write the above equation as\n",
    "\n",
    "$$\n",
    "Q(s,a) = r(s,a) + \\max_{a'\\in\\mathcal A}Q(s',a').\n",
    "$$\n",
    "\n",
    "At every time step $t$, we choose an action that gives the maximum action-value, in which case the policy is called greedy with respect to $Q(s,a)$. Following the policy, we store the experiences (the states, actions, rewards, etc.). When the episode is over, we update the action-value function to some new $Q'(s,a)$:\n",
    "\n",
    "$$\n",
    "Q'(s,a) = (1-\\alpha)Q(s,a) + \\alpha\\left( r(s,a) + \\max_{a'\\in\\mathcal A} Q(s',a') \\right).\n",
    "$$\n",
    "\n",
    "for some constant $\\alpha$. \n",
    "\n",
    "However, if the agent always chooses the action that maximises the current estimate of the action-value function, it may lose chances to discover better actions that could have given higher rewards. There is a trade-off between exploration and exploitation.\n",
    "\n",
    "Explotation refers to choosing the best possible actions based on what the agent has learned so far. On the other hand, exploration means trying out new actions that may lead to discover higher rewards. Without sufficient exploration, the agent might converge to a suboptimal policy, misisng out on better strategies.\n",
    "\n",
    "One approach to balance exploration and exploitation is the $\\epsilon$-greedy strategy ([Sutton and Barto (2018)]()). The agent randomly chooses any action with pre-specified probability $\\epsilon$ for exploration. In this case we have\n",
    "\n",
    "$$\n",
    "a_t = \n",
    "\\begin{cases} \n",
    "\\arg\\max_{a\\in\\mathcal A}Q_t(s,a) & \\text{with probability } 1-\\epsilon\n",
    "\\\\ \n",
    "\\text{random action from }\\mathcal A & \\text{with probability } \\epsilon\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network\n",
    "\n",
    "In Q-learning, we store the values for every possible state-action pair in a table called Q-table. However, the number of states and actions grow exponentially as the number of dimensions increases, making the task of finding the optimal policy computationally challenging.\n",
    "\n",
    "[Mnih, Kavukcuoglu, et al. (2015)]() suggested that we approximate the action-value function with a neural network, instead of iterating over states whose dimension can be very large. That is, we paremeterise the action-value function $Q(s,a)$ by some parameters $\\theta$  and use $Q(s,a;\\theta) instead.\n",
    "\n",
    "As Tsitsiklis and Van Roy (1996) pointed out, using a nonlinear function approximator such as a neural network can make learning unstable or to diverge. This happens when the sequence of observations have high correlations or when the small updates in the action-value function significantly changes the policy and therefore affects the distribution of the data. To address this issue, DQN uses two ideas: expereince reply ([Lin (1992)]()) and separating the behaviour and the target network.\n",
    "\n",
    "#### Experience Replay\n",
    "We store the experience of the agent at time step $t$, $(s_t, a_t, r_t, s_{t+1})$, in the replay memory $\\mathcal D$. During training, we randomly sample a mini-batch of samples $(s_i, a_i, r_i, s_i')$ of size $B$ for some pre-specified batch size $B$ from the memory to train the network. If the size of the replay memory is large enough, we can obtain uncorrelated and nearly independent samples. This reduces the variance of the gradient $\\nabla_\\theta L(\\theta)$ and stabilises the training process.\n",
    "\n",
    "#### Behaviour and Target Networks\n",
    "We utilise two neural networks having identical structures: a behaviour network and a target network. The behaviour network, parameterised by $\\theta$, calculates the action-value of the current time step and is updated by gradient descent method:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta-\\nabla_\\theta L(\\theta).\n",
    "$$\n",
    "\n",
    "The target network, parameterised by $\\theta'$, are periodically updated every $C$ steps, where $C$ is a hyperparameter:\n",
    "\n",
    "$$\n",
    "\\theta' = (1-\\tau)\\theta' + \\tau\\theta\n",
    "$$\n",
    "\n",
    "for another hyperparameter $tau$. \n",
    "\n",
    "Our loss function, therefore, is\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{B}\\sum_{i=1}^B\\left( (r_i + \\max_{a'\\in\\mathcal A}Q(s_i', a';\\theta')) - Q(s_i,a_i;\\theta) \\right).\n",
    "$$\n",
    "\n",
    "Note that the target network is only used to calculate $r+\\max_{a'\\in\\mathcal A}Q(s_i', a';\\theta')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dqn-algorithm](../../figures/dqn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_hidden=64):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, n_actions, memory_size, batch_size):\n",
    "        self.n_actions = n_actions\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, n_states, n_actions, batch_size=64, lr=1e-4, gamma=0.99, memory_size=int(1e5), tau=1e-3, learn_step=5):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.memory_size = memory_size\n",
    "        self.tau = tau\n",
    "        self.learn_step = learn_step\n",
    "\n",
    "        # model\n",
    "        self.net_eval = QNet(n_states, n_actions).to(device)\n",
    "        self.net_target = QNet(n_states, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net_eval.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayBuffer(n_actions, memory_size, batch_size)\n",
    "        self.counter = 0 # Update cycle counter\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # print(state)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        self.net_eval.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.net_eval(state)\n",
    "        self.net_eval.train()\n",
    "\n",
    "        # epsilon-greedy policy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(np.arange(self.n_actions))\n",
    "        else:\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def save_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.counter += 1\n",
    "        if self.counter % self.learn_step == 0:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "    \n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_target = self.net_target(next_states).detach().max(axis=1)[0].unsqueeze(1)\n",
    "        y_j = rewards + self.gamma * (1 - dones) * q_target\n",
    "        q_eval = self.net_eval(states).gather(1, actions)\n",
    "\n",
    "        # loss backpropagation\n",
    "        loss = self.criterion(q_eval, y_j)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, eval_param in zip(self.net_target.parameters(), self.net_eval.parameters()):\n",
    "            target_param.data.copy_(self.tau * eval_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=2000, max_steps=1000, eps_start=1.0, eps_end=0.1, eps_decay=0.995, target=200):\n",
    "    score_history = []\n",
    "    epislon = eps_start\n",
    "\n",
    "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
    "    progress_bar = tqdm.trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
    "\n",
    "    for i in progress_bar:\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        score = 0\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epislon)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.save_to_memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        score_history.append(score)\n",
    "        score_avg = np.mean(score_history[-100:])\n",
    "        epislon = max(epislon * eps_decay, eps_end)\n",
    "\n",
    "        progress_bar.set_postfix_str(f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}\")\n",
    "        progress_bar.update(0)\n",
    "\n",
    "        # Early stopping\n",
    "        if len(score_history) > 100:\n",
    "            if score_avg >= target:\n",
    "                break\n",
    "        \n",
    "    if (i + 1) < n_episodes:\n",
    "        print(\"\\nTarget score reached!\")\n",
    "    else:\n",
    "        print(\"\\nDone!\")\n",
    "\n",
    "    torch.save(agent.net_eval.state_dict(), f\"./dqn-trained.h5\")\n",
    "\n",
    "    return score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPISODES = 5000\n",
    "TARGET_SCORE = 250.     # early training stop at avg score of last 100 episodes\n",
    "GAMMA = 0.99            # discount factor\n",
    "MEMORY_SIZE = 10000     # max memory buffer size\n",
    "LEARN_STEP = 5          # how often to learn\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "\n",
    "train_env = gym.make('LunarLander-v3')\n",
    "num_states = train_env.observation_space.shape[0]\n",
    "num_actions = train_env.action_space.n\n",
    "agent = DQN(\n",
    "    n_states = num_states,\n",
    "    n_actions = num_actions,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    lr = LR,\n",
    "    gamma = GAMMA,\n",
    "    memory_size = MEMORY_SIZE,\n",
    "    learn_step = LEARN_STEP,\n",
    "    tau = TAU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_hist = train(train_env, agent, n_episodes=EPISODES, target=TARGET_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training history as csv file\n",
    "\n",
    "df = pd.DataFrame(score_hist, columns=[\"score\"])\n",
    "df.to_csv(\"dqn-score-history.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define visualisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScore(scores):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Score History\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(\"dqn-score-history.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise training history\n",
    "plotScore(score_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, agent, loop=3):\n",
    "    for i in range(loop):\n",
    "        state, _ = env.reset()\n",
    "        for t in range(500):\n",
    "            action = agent.act(state, 0)\n",
    "            env.render()\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    env.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "GAMMA = 0.99            # discount factor\n",
    "MEMORY_SIZE = 10000     # max memory buffer size\n",
    "LEARN_STEP = 5          # how often to learn\n",
    "TAU = 1e-3              # for soft update of target parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make('LunarLander-v3', render_mode=\"human\")\n",
    "num_states = test_env.observation_space.shape[0]\n",
    "num_actions = test_env.action_space.n\n",
    "agent = DQN(\n",
    "    n_states = num_states,\n",
    "    n_actions = num_actions,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    lr = LR,\n",
    "    gamma = GAMMA,\n",
    "    memory_size = MEMORY_SIZE,\n",
    "    learn_step = LEARN_STEP,\n",
    "    tau = TAU\n",
    ")\n",
    "\n",
    "# Load the trained agent\n",
    "agent.net_eval.load_state_dict(torch.load(f'./dqn-trained.h5', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_env, agent, loop=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
