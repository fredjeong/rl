{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor-Critic on LunarLanderContinuous-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project delves into a famous deep reinforcement learning algorithm, the Advantage Actor-Critic (A2C) Algorithm. We first cover the algorithm's underlying theory, followed by its implementation in PyTorch. Then we train and evaluate the reinforcement learning agent within OpenAI's LunarLanderConitnuous-v3 environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theoretical Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient\n",
    "\n",
    "Let $J(\\theta)$ denote the expectation of our cumulative reward over the entire episode with a finite time horizon $T$ given policy $\\pi_\\theta$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb E\\left[ \\sum_{t=1}^T \\gamma ^{t-1}r(s_t,a_t) \\mid \\pi_\\theta, s_1 \\right]\n",
    "$$\n",
    "\n",
    "for some discount factor $\\gamma$. Our objective is to find optimal set of parameters $\\theta^*$ such that $\\theta^*=\\arg\\max_\\theta J(\\theta)$. \n",
    "\n",
    "Let $D^{\\pi_\\theta}(\\tau)$ denote the probability distribution of a trajectory $\\tau=(s_1, a_1, \\cdots, s_{T-1}, a_{T-1}, s_T)$. That is,\n",
    "\n",
    "$$\n",
    "D^{\\pi_\\theta}(\\tau) = \\prod_{i=1}^{T-1} \\pi_\\theta(a_i\\mid s_i) P(s_{i+1}\\mid s_i, a_i).\n",
    "$$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb E_{\\tau\\sim D^{\\pi_\\theta}}[R(\\tau)]\n",
    "$$\n",
    "\n",
    "where $R(\\tau)=\\sum_{t=1}^{T-1}\\gamma^{t-1}R(s_t, a_t)$ is the expected total reward over the entire trajectory $\\tau$. Note that $R(s_t, a_t)$ is the expectation of the reward given $s_t$ and $a_t$.\n",
    "\n",
    "Under finite state and action spaces, the distribution $D(\\tau)$ has a finite support. Assuming that our policy $\\pi_\\theta$ is differentiable with respect to $\\theta$, we have \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) \n",
    "&= \\nabla_\\theta \\sum_{\\tau:D^{\\pi_\\theta}>0}D^{\\pi_\\theta}(\\tau) R(\\tau)\n",
    "\\\\\n",
    "&= \\sum_{\\tau:D^{\\pi_\\theta}>0}D^{\\pi_\\theta}(\\tau) \\frac{\\nabla_\\theta D^{\\pi_\\theta}(\\tau)}{D^{\\pi_\\theta}(\\tau)}R(\\tau)\n",
    "\\\\\n",
    "&= \\sum_{\\tau:D^{\\pi_\\theta}>0}D^{\\pi_\\theta}(\\tau) \\nabla_\\theta \\log D^{\\pi_\\theta}(\\tau) R(\\tau)\n",
    "\\\\\n",
    "&= \\mathbb E_{\\tau\\sim D^{\\pi_\\theta}}\\left[ \\nabla_\\theta \\log D^{\\pi_\\theta}(\\tau) R(\\tau) \\right].\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For any given trajectory, we then have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta \\log D^{\\pi_\\theta}(\\tau) \n",
    "&= \\nabla_\\theta \\log \\left( \\prod_{t=1}^{T-1} \\pi_\\theta(s_t, a_t) P(s_{t+1} \\mid s_t, a_t) \\right)\n",
    "\\\\\n",
    "&= \\sum_{t=1}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) + \\sum_{t=1}^{T-1}\\nabla_\\theta P(s_{t+1}\\mid s_t, a_t)\n",
    "\\\\\n",
    "&= \\sum_{t=1}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the derivative of the transition function $P(s_{t+1} \\mid s_t, a_t)$ is zero.\n",
    "\n",
    "Given a trajectory $\\tau$, the quantity $D^{\\pi_\\theta}(\\tau)$ is determined and we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) \n",
    "&= \\mathbb E_\\tau \\left[ R(\\tau) \\nabla_\\theta \\log(D^{\\pi_\\theta}(\\tau)) \\right]\n",
    "\\\\\n",
    "&= \\mathbb E_\\tau \\left[ R(\\tau) \\sum_{t=1}^{T-1} \\nabla_\\theta \\log (\\pi_\\theta(a_t\\mid s_t)) \\right].\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "or, as the Policy Gradient Theorem ([Sutton, McAllester, et al. (1999)]()) suggests,\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb E_{\\pi_\\theta} \\left[Q^{\\pi_\\theta}(s,a) \\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\right].\n",
    "$$\n",
    "\n",
    "\n",
    "The update rule is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic\n",
    "\n",
    "Vanilla policy gradient method suffers from high variance, meaning that the updates to the policy can be noisy and unstable. This high variance arises because the updates rely on estimating the total future reward, which can vary significantly from one episode to another.\n",
    "\n",
    "An actor-critic methods approaches this issue by utilising two different networks. Straightforward from their names, the actor network decides which action to take and the critic network evaluates the actions taken by the actor network by estimating the value of the current state. \n",
    "\n",
    "We approximate the action-value function $Q^{\\pi_\\theta}(s,a)$ by some function approximator $Q_w(s,a)$ (a neural network with a set of parameters $w$). So that in the training process, we update two networks simultaneously. The actor and the critic give feedbacks to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic\n",
    "\n",
    "A further way to reduce variance is using a \"baseline\" and subtract it from the policy gradient. We now figure out how this can reduce variance without changing expectation.\n",
    "\n",
    "For any function of the state $b(s)$ we have \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb E_{\\pi_\\theta}\\left[ b(s)\\nabla_\\theta \\log \\pi_\\theta(s,a) \\right]\n",
    "&= \\int_{s\\in\\mathcal S}D^{\\pi_\\theta}(s)\\int_{a\\in\\mathcal A} b(s)\\nabla_\\theta \\pi_\\theta(a \\mid s)\n",
    "\\\\\n",
    "&= \\int_{s\\in\\mathcal S}D^{\\pi_\\theta}(s) b(s)\\nabla_\\theta\\int_{a\\in\\mathcal A}\\pi_\\theta(a\\mid s)\n",
    "\\\\\n",
    "&= \\int_{s\\in\\mathcal S}D^{\\pi_\\theta}(s) b(s)\\nabla_\\theta 1\n",
    "\\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathcal S$ is the state space and $\\mathcal A$ is the action space. As the expectation is zero, subtracting the baseline function does not change expectation.\n",
    "\n",
    "Although it is different than the optimal baseline suggested by [Williams, 1992](), often we use $V^{\\pi_\\theta}(s)$ for our baseline. By choosing $b(s)=V(s)$ we define the advantage function $A^{\\pi_\\theta}$ such that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^{\\pi_\\theta}(s,a) \n",
    "&= Q^{\\pi_\\theta}(s,a) - V^{\\pi_\\theta}(s)\n",
    "\\\\\n",
    "&= r(s,a) + V^{\\pi_\\theta}(s') - V^{\\pi_\\theta}(s)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and we have \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "&= \\mathbb E_{\\pi_\\theta} \\left[ A^{\\pi_\\theta}(s,a) \\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\right]\n",
    "\\\\\n",
    "&= \\mathbb E_{\\pi_\\theta} \\left[ \\left( r(s,a) + V^{\\pi_\\theta}(s') - V^{\\pi_\\theta}(s) \\right) \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\right]\n",
    "\\\\\n",
    "&= \\mathbb E_{\\pi_\\theta} \\left[ \\left( r(s,a) + V_w(s') - V_w(s) \\right) \\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where in the last part we approximated the value function by a neural network with a set of parameters $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a2c-algorithm](../../figures/a2c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define actor and critic networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define A2C agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
